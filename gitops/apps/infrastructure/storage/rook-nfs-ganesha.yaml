# Rook-based NFS-Ganesha deployment for Kubernetes cluster
# This provides native CephFS NFS exports using Rook operator
# Replaces manual NFS-Ganesha deployment with CRD-based management
# Updated: 2025-06-19

# Install Rook Operator using official Helm chart
---
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: rook-ceph-operator
  namespace: argocd
  annotations:
    argocd.argoproj.io/sync-wave: "1"
spec:
  project: default
  source:
    repoURL: https://charts.rook.io/release
    chart: rook-ceph    
    targetRevision: v1.15.1
    helm:
      values: |
        # Rook operator configuration
        image:
          repository: rook/ceph
          tag: v1.15.1
        nodeSelector: {}
        tolerations: []
        resources:
          limits:
            cpu: 500m
            memory: 512Mi
          requests:
            cpu: 100m
            memory: 128Mi
        # Enable CRDs installation
        crds:
          enabled: true        
        # Monitoring configuration
        monitoring:
          enabled: false
        # CSI configuration
        csi:
          enableRbdDriver: true
          enableCephfsDriver: true
          enableNFSSnapshotter: true        
        # Enable discovery daemon
        enableDiscoveryDaemon: false  
  destination:
    server: https://kubernetes.default.svc
    namespace: rook-ceph  
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
    managedNamespaceMetadata:
      labels:
        pod-security.kubernetes.io/enforce: privileged
        pod-security.kubernetes.io/audit: privileged
        pod-security.kubernetes.io/warn: privileged    
    syncOptions:
    - CreateNamespace=true
    - Replace=true

---
# ConfigMap for external cluster configuration
apiVersion: v1
kind: ConfigMap
metadata:
  name: rook-ceph-mon-endpoints
  namespace: rook-ceph
  annotations:
    argocd.argoproj.io/sync-wave: "1"
data:
  data: "pve-r640-01=192.168.40.60:6789,pve-r740xd-01=192.168.40.61:6789,pve-r740xd-02=192.168.40.62:6789"
  mapping: |
    {
      "node": {
        "pve-r640-01": {
          "Name": "pve-r640-01",
          "Addrs": {
            "addrvec": [
              {
                "addr": "192.168.40.60:6789",
                "nonce": 0,
                "type": "legacy"
              }
            ]
          }
        },
        "pve-r740xd-01": {
          "Name": "pve-r740xd-01", 
          "Addrs": {
            "addrvec": [
              {
                "addr": "192.168.40.61:6789",
                "nonce": 0,
                "type": "legacy"
              }
            ]
          }
        },
        "pve-r740xd-02": {
          "Name": "pve-r740xd-02",
          "Addrs": {
            "addrvec": [
              {
                "addr": "192.168.40.62:6789", 
                "nonce": 0,
                "type": "legacy"
              }
            ]
          }
        }
      }
    }
  maxMonId: "2"

---
# ConfigMap for cluster ID
apiVersion: v1
kind: ConfigMap
metadata:
  name: rook-ceph-cluster-info
  namespace: rook-ceph
  annotations:
    argocd.argoproj.io/sync-wave: "1"
data:
  cluster-id: "310985e9-91ef-4f2f-a6e6-89327b2a8b1d"  # FSID from your ceph.conf

---
# CephCluster for external Proxmox Ceph connection
apiVersion: ceph.rook.io/v1
kind: CephCluster
metadata:
  name: rook-ceph
  namespace: rook-ceph
  annotations:
    argocd.argoproj.io/sync-wave: "2"
spec:
  external:
    enable: true
  # Disable components we don't need since we're using external cluster
  crashCollector:
    disable: true
  cephBlockPools: []     # Managed by external cluster
  cephFileSystems: []    # Managed by external cluster  
  cephObjectStores: []   # Not needed for NFS setup
  cleanupPolicy:
    confirmation: ""
    allowUninstallWithVolumes: false
  dataDirHostPath: /var/lib/rook
  # Health checks for external cluster
  healthCheck:
    daemonHealth:
      mon:
        disabled: false
        interval: 60s  
  cephVersion:
    image: quay.io/ceph/ceph:v19.2.0
    allowUnsupported: false
  # Skip dashboard and monitoring for minimal setup
  dashboard:
    enabled: false
  monitoring:
    enabled: false

---
# CephNFS for media exports (deployed after cluster is ready)
apiVersion: ceph.rook.io/v1
kind: CephNFS
metadata:
  name: media-nfs
  namespace: rook-ceph
  annotations:
    argocd.argoproj.io/sync-wave: "3"
spec:
  rados:
    pool: cephfs_metadata  # Use existing CephFS metadata pool from Proxmox
    namespace: nfs-ganesha
  server:
    active: 2  # High availability
    resources:
      limits:
        cpu: 1000m
        memory: 2Gi
      requests:
        cpu: 500m
        memory: 1Gi
    logLevel: INFO
    # Placement on worker nodes
    placement:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: node-role.kubernetes.io/control-plane
              operator: DoesNotExist

---
# Service to expose Rook NFS-Ganesha externally via MetalLB
# This provides the LoadBalancer IP that media workloads will use
apiVersion: v1
kind: Service
metadata:
  name: rook-nfs-external
  namespace: rook-ceph
  annotations:
    argocd.argoproj.io/sync-wave: "4"
    # MetalLB annotations for better control
    metallb.universe.tf/address-pool: default-pool
    metallb.universe.tf/allow-shared-ip: "false"
  labels:
    app: rook-ceph-nfs-media-nfs
spec:
  selector:
    app: rook-ceph-nfs-media-nfs  # Rook's default label for CephNFS pods
    nfs_cluster: media-nfs
  ports:
  - name: nfs
    port: 2049
    protocol: TCP
    targetPort: 2049
  - name: mountd
    port: 20048
    protocol: TCP
    targetPort: 20048
  - name: rpcbind
    port: 111
    protocol: TCP
    targetPort: 111  
  type: LoadBalancer
  # Let MetalLB auto-assign IP from the pool instead of hardcoding
  # loadBalancerIP: 192.168.55.220

---
# ClusterIP Service for internal cluster access
apiVersion: v1
kind: Service
metadata:
  name: rook-nfs-internal
  namespace: rook-ceph
  annotations:
    argocd.argoproj.io/sync-wave: "4"
  labels:
    app: rook-ceph-nfs-media-nfs
spec:
  selector:
    app: rook-ceph-nfs-media-nfs
    nfs_cluster: media-nfs
  ports:
  - name: nfs
    port: 2049
    protocol: TCP
    targetPort: 2049  
  - name: mountd
    port: 20048
    protocol: TCP
    targetPort: 20048
  - name: rpcbind
    port: 111
    protocol: TCP
    targetPort: 111
  type: ClusterIP

---
# ConfigMap with NFS export configuration
# Note: In Rook v1.15.1, NFS exports are typically configured via ConfigMap
# and mounted into the NFS-Ganesha pods, or managed externally
apiVersion: v1
kind: ConfigMap
metadata:
  name: nfs-ganesha-config
  namespace: rook-ceph
  annotations:
    argocd.argoproj.io/sync-wave: "5"
data:
  ganesha.conf: |
    EXPORT
    {
        Export_Id = 1;
        Path = /;
        Pseudo = /media;
        Access_Type = RW;
        Squash = no_root_squash;
        Protocols = 4;
        Transports = TCP;
        FSAL {
            Name = CEPH;
            Filesystem = "pve";
            User_Id = "admin";
            Secret_Access_Key = "";
        }
        CLIENT {
            Clients = *;
            Access_Type = RW;
        }
    }
