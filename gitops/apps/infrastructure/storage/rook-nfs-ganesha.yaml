# Rook-based NFS-Ganesha deployment for Kubernetes cluster
# This provides native CephFS NFS exports using Rook operator
# Replaces manual NFS-Ganesha deployment with CRD-based management
# Updated: 2025-06-19

# Install Rook Operator using official Helm chart
---
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: rook-ceph-operator
  namespace: argocd
  annotations:
    argocd.argoproj.io/sync-wave: "1"
spec:
  project: default
  source:
    repoURL: https://charts.rook.io/release
    chart: rook-ceph    
    targetRevision: v1.15.1
    helm:
      values: |
        # Rook operator configuration
        image:
          repository: rook/ceph
          tag: v1.15.1
        nodeSelector: {}
        tolerations: []
        resources:
          limits:
            cpu: 500m
            memory: 512Mi
          requests:
            cpu: 100m
            memory: 128Mi
        # Enable CRDs installation
        crds:
          enabled: true        
        # Monitoring configuration
        monitoring:
          enabled: false
        # CSI configuration
        csi:
          enableRbdDriver: true
          enableCephfsDriver: true
          enableNFSSnapshotter: true        
        # Enable discovery daemon
        enableDiscoveryDaemon: false  
  destination:
    server: https://kubernetes.default.svc
    namespace: rook-ceph
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
    managedNamespaceMetadata:
      labels:
        pod-security.kubernetes.io/enforce: privileged
        pod-security.kubernetes.io/audit: privileged
        pod-security.kubernetes.io/warn: privileged
    syncOptions:
    - CreateNamespace=true
    - Replace=true

---
# External cluster connection secrets (using Infisical)
apiVersion: v1
kind: Namespace
metadata:
  name: rook-ceph
  annotations:
    argocd.argoproj.io/sync-wave: "0"
  labels:
    name: rook-ceph
    pod-security.kubernetes.io/enforce: privileged
    pod-security.kubernetes.io/audit: privileged
    pod-security.kubernetes.io/warn: privileged

---
# Infisical Secret for Rook external cluster access
apiVersion: secrets.infisical.com/v1alpha1
kind: InfisicalSecret
metadata:
  name: rook-ceph-external-secret
  namespace: rook-ceph
  annotations:
    argocd.argoproj.io/sync-wave: "1"
spec:
  secretsPath: "/ceph"
  projectId: "f492c339-2822-40bd-84cb-6f32326a3a38"
  envSlug: "prod"
  secretType: Opaque
  resyncInterval: 60
  managedSecretReference:
    secretName: rook-ceph-mon
    secretNamespace: rook-ceph
    creationPolicy: "Owner"
  authentication:
    universalAuth:
      secretsScope:
        projectId: "f492c339-2822-40bd-84cb-6f32326a3a38"
        projectSlug: "ombra-mi-wk"
        envSlug: "prod"
        secretsPath: "/ceph"
      credentialsRef:
        secretName: infisical-universal-auth
        secretNamespace: infisical-operator-system

---
# ConfigMap for external cluster configuration
apiVersion: v1
kind: ConfigMap
metadata:
  name: rook-ceph-mon-endpoints
  namespace: rook-ceph
  annotations:
    argocd.argoproj.io/sync-wave: "1"
data:
  data: "pve-r640-01=192.168.40.60:6789,pve-r740xd-01=192.168.40.61:6789,pve-r740xd-02=192.168.40.62:6789"
  mapping: |
    {
      "node": {
        "pve-r640-01": {
          "Name": "pve-r640-01",
          "Addrs": {
            "addrvec": [
              {
                "addr": "192.168.40.60:6789",
                "nonce": 0,
                "type": "legacy"
              }
            ]
          }
        },
        "pve-r740xd-01": {
          "Name": "pve-r740xd-01", 
          "Addrs": {
            "addrvec": [
              {
                "addr": "192.168.40.61:6789",
                "nonce": 0,
                "type": "legacy"
              }
            ]
          }
        },
        "pve-r740xd-02": {
          "Name": "pve-r740xd-02",
          "Addrs": {
            "addrvec": [
              {
                "addr": "192.168.40.62:6789", 
                "nonce": 0,
                "type": "legacy"
              }
            ]
          }
        }
      }
    }  maxMonId: "2"

---
# ConfigMap for cluster ID
apiVersion: v1
kind: ConfigMap
metadata:
  name: rook-ceph-cluster-info
  namespace: rook-ceph
  annotations:
    argocd.argoproj.io/sync-wave: "1"
data:
  cluster-id: "310985e9-91ef-4f2f-a6e6-89327b2a8b1d"  # FSID from your ceph.conf

---
# CephCluster for external Proxmox Ceph connection
apiVersion: ceph.rook.io/v1
kind: CephCluster
metadata:
  name: rook-ceph
  namespace: rook-ceph
  annotations:
    argocd.argoproj.io/sync-wave: "2"
spec:
  external:
    enable: true
  # Disable components we don't need since we're using external cluster
  crashCollector:
    disable: true
  cephBlockPools: []     # Managed by external cluster
  cephFileSystems: []    # Managed by external cluster  
  cephObjectStores: []   # Not needed for NFS setup
  cleanupPolicy:
    confirmation: ""
    allowUninstallWithVolumes: false
  dataDirHostPath: /var/lib/rook
  # Health checks for external cluster
  healthCheck:
    daemonHealth:
      mon:
        disabled: false
        interval: 60s
  cephVersion:
    image: quay.io/ceph/ceph:v19.2.0
    allowUnsupported: false
  # Skip dashboard and monitoring for minimal setup
  dashboard:
    enabled: false
  monitoring:
    enabled: false

---
# CephFilesystem reference for external filesystem
apiVersion: ceph.rook.io/v1
kind: CephFilesystem
metadata:
  name: pve  # This matches your external filesystem name from MDS config
  namespace: rook-ceph
  annotations:
    argocd.argoproj.io/sync-wave: "2"
spec:
  # For external clusters, we just need to reference the existing filesystem
  preserveFilesystemOnDelete: true
  metadataServer:
    activeCount: 0  # External cluster manages MDS
    activeStandby: false
  dataPools: []  # External cluster manages pools

---
# CephNFS for media exports (deployed after cluster is ready)
apiVersion: ceph.rook.io/v1
kind: CephNFS
metadata:
  name: media-nfs
  namespace: rook-ceph
  annotations:
    argocd.argoproj.io/sync-wave: "3"
spec:
  rados:
    pool: cephfs_metadata  # Use existing CephFS metadata pool from Proxmox
    namespace: nfs-ganesha
  server:
    active: 2  # High availability
    resources:
      limits:
        cpu: 1000m
        memory: 2Gi
      requests:
        cpu: 500m
        memory: 1Gi
    logLevel: INFO
    # Placement on worker nodes
    placement:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: node-role.kubernetes.io/control-plane
              operator: DoesNotExist

---
# CephFilesystemSubVolumeGroup for organizing NFS exports
apiVersion: ceph.rook.io/v1
kind: CephFilesystemSubVolumeGroup
metadata:
  name: media-subvolume-group
  namespace: rook-ceph
  annotations:
    argocd.argoproj.io/sync-wave: "3"
spec:
  filesystemName: pve  # Use existing CephFS name from Proxmox
  pinning:
    distributed: 1

---
# NFS Export for media directory
apiVersion: ceph.rook.io/v1
kind: CephNFSExport
metadata:
  name: media-export
  namespace: rook-ceph
  annotations:
    argocd.argoproj.io/sync-wave: "4"
spec:
  nfsCluster: media-nfs
  exportSpec:
    exportId: 1
    path: /media
    pseudo: /media
    access_type: RW
    squash: no_root_squash
    protocols:
      - 4
    transports:
      - TCP    
    fsal:
      name: CEPH
      filesystem: pve
      path: /volumes/media-subvolume-group/media-share
    clients: []  # Allow access from any client

---
# Subvolume for media NFS share
apiVersion: ceph.rook.io/v1
kind: CephFilesystemSubVolume
metadata:
  name: media-share
  namespace: rook-ceph
  annotations:
    argocd.argoproj.io/sync-wave: "3"
spec:
  filesystemName: pve
  subVolumeGroupName: media-subvolume-group
  size: "100Gi"  # Adjust as needed

---
# Service to expose Rook NFS-Ganesha externally via MetalLB
# This provides the LoadBalancer IP that media workloads will use
apiVersion: v1
kind: Service
metadata:
  name: rook-nfs-external
  namespace: rook-ceph
  annotations:
    argocd.argoproj.io/sync-wave: "4"
    # MetalLB annotations for better control
    metallb.universe.tf/address-pool: default-pool
    metallb.universe.tf/allow-shared-ip: "false"
  labels:
    app: rook-ceph-nfs-media-nfs
spec:
  selector:
    app: rook-ceph-nfs-media-nfs  # Rook's default label for CephNFS pods
    nfs_cluster: media-nfs
  ports:
  - name: nfs
    port: 2049
    protocol: TCP
    targetPort: 2049
  - name: mountd
    port: 20048
    protocol: TCP
    targetPort: 20048
  - name: rpcbind
    port: 111
    protocol: TCP
    targetPort: 111  
  type: LoadBalancer
  # Let MetalLB auto-assign IP from the pool instead of hardcoding
  # loadBalancerIP: 192.168.55.220

---
# ClusterIP Service for internal cluster access
apiVersion: v1
kind: Service
metadata:
  name: rook-nfs-internal
  namespace: rook-ceph
  annotations:
    argocd.argoproj.io/sync-wave: "4"
  labels:
    app: rook-ceph-nfs-media-nfs
spec:
  selector:
    app: rook-ceph-nfs-media-nfs
    nfs_cluster: media-nfs
  ports:
  - name: nfs
    port: 2049
    protocol: TCP
    targetPort: 2049
  - name: mountd
    port: 20048
    protocol: TCP
    targetPort: 20048
  - name: rpcbind
    port: 111
    protocol: TCP
    targetPort: 111
  type: ClusterIP

---
# Job to initialize media directory structure
apiVersion: batch/v1
kind: Job
metadata:
  name: init-media-directories
  namespace: rook-ceph
  annotations:
    argocd.argoproj.io/sync-wave: "5"
spec:
  template:
    spec:
      restartPolicy: OnFailure
      containers:
      - name: init-dirs
        image: busybox:1.36
        command:
        - sh
        - -c
        - |
          set -e
          echo "Creating media directory structure..."
          
          # Create main media directories
          mkdir -p /media/downloads/{complete,incomplete,watch}
          mkdir -p /media/media/{movies,tv,music,books}
          mkdir -p /media/torrents/{movies,tv,music,books}
          
          # Set permissions (assuming UID/GID 2000 for media apps)
          chown -R 2000:2000 /media
          chmod -R 755 /media
          
          echo "Media directory structure created successfully!"
          ls -la /media/        
        volumeMounts:
        - name: media-storage
          mountPath: /media
        securityContext:
          runAsUser: 0  # Run as root to set ownership
          runAsGroup: 0
      securityContext:
        fsGroup: 2000
      volumes:
      - name: media-storage
        persistentVolumeClaim:
          claimName: media-cephfs-pvc

---
# PVC for media storage using CephFS subvolume
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: media-cephfs-pvc
  namespace: rook-ceph
  annotations:
    argocd.argoproj.io/sync-wave: "4"
spec:
  accessModes:
  - ReadWriteMany
  resources:
    requests:
      storage: 100Gi  # Should match subvolume size
  storageClassName: ceph-cephfs
  volumeMode: Filesystem
