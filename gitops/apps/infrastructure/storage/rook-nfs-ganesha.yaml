# Rook-based NFS-Ganesha deployment for Kubernetes cluster
# This provides native CephFS NFS exports using Rook operator
# Replaces manual NFS-Ganesha deployment with CRD-based management
# Updated: 2025-06-19

# Install Rook Operator using official Helm chart
---
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: rook-ceph-operator
  namespace: argocd
spec:
  project: default
  source:
    repoURL: https://charts.rook.io/release
    chart: rook-ceph
    targetRevision: v1.14.11
    helm:
      values: |
        # Rook operator configuration
        image:
          repository: rook/ceph
          tag: v1.14.11
        nodeSelector: {}
        tolerations: []
        resources:
          limits:
            cpu: 500m
            memory: 512Mi
          requests:
            cpu: 100m
            memory: 128Mi
        # Enable CRDs installation
        crds:
          enabled: true        
        # Monitoring configuration
        monitoring:
          enabled: false
        # CSI configuration
        csi:
          enableRbdDriver: true
          enableCephfsDriver: true
          enableNFSSnapshotter: true        
        # Enable discovery daemon
        enableDiscoveryDaemon: false
  destination:
    server: https://kubernetes.default.svc
    namespace: rook-ceph
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
    managedNamespaceMetadata:
      labels:
        pod-security.kubernetes.io/enforce: privileged
        pod-security.kubernetes.io/audit: privileged
        pod-security.kubernetes.io/warn: privileged
    syncOptions:
    - CreateNamespace=true
    - Replace=true

---
# Wait for operator to be ready before deploying CephCluster
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: rook-ceph-cluster
  namespace: argocd
spec:
  project: default
  source:
    repoURL: https://charts.rook.io/release
    chart: rook-ceph-cluster
    targetRevision: v1.14.11
    helm:
      values: |
        # External Ceph cluster configuration
        operatorNamespace: rook-ceph
        configOverride: |
          [global]
          osd_pool_default_size = 3
          osd_pool_default_min_size = 2
        cephClusterSpec:
          external:
            enable: true
          crashCollector:
            disable: true
          healthCheck:
            daemonHealth:
              mon:
                disabled: false
                interval: 45s
              osd:
                disabled: false
                interval: 60s
              status:
                disabled: false
                interval: 60s
          cephVersion:
            image: quay.io/ceph/ceph:v18.2.0
            allowUnsupported: false
        # Ingress configuration
        ingress:
          dashboard:
            enabled: false
        # Monitoring configuration  
        monitoring:
          enabled: false
          createPrometheusRules: false
        # CSI configuration
        cephBlockPools:
        - name: ceph-blockpool
          spec:
            failureDomain: host
            replicated:
              size: 3
          storageClass:
            enabled: true
            name: ceph-rbd
            isDefault: false
            reclaimPolicy: Retain
            allowVolumeExpansion: true
        cephFileSystems:
        - name: media-filesystem
          spec:
            metadataPool:
              replicated:
                size: 3
            dataPools:
            - name: data0
              replicated:
                size: 3
            preserveFilesystemOnDelete: true
            metadataServer:
              activeCount: 1
              activeStandby: true
              resources:
                limits:
                  cpu: 2000m
                  memory: 4Gi
                requests:
                  cpu: 1000m
                  memory: 4Gi
          storageClass:
            enabled: true
            name: rook-cephfs
            isDefault: false
            reclaimPolicy: Retain
            allowVolumeExpansion: true
        cephObjectStores: []  
  destination:
    server: https://kubernetes.default.svc
    namespace: rook-ceph
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
    managedNamespaceMetadata:
      labels:
        pod-security.kubernetes.io/enforce: privileged
        pod-security.kubernetes.io/audit: privileged
        pod-security.kubernetes.io/warn: privileged
    syncOptions:
    - CreateNamespace=true

---
# CephNFS for media exports
apiVersion: ceph.rook.io/v1
kind: CephNFS
metadata:
  name: media-nfs
  namespace: rook-ceph
spec:
  rados:
    pool: media-filesystem-metadata
    namespace: nfs-ganesha
  server:
    active: 2  # High availability
    resources:
      limits:
        cpu: 1000m
        memory: 2Gi
      requests:
        cpu: 500m
        memory: 1Gi
    logLevel: INFO
    # Placement on worker nodes
    placement:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: node-role.kubernetes.io/control-plane
              operator: DoesNotExist

---
# CephFilesystemSubVolumeGroup for organizing NFS exports
apiVersion: ceph.rook.io/v1
kind: CephFilesystemSubVolumeGroup
metadata:
  name: media-subvolume-group
  namespace: rook-ceph
spec:
  filesystemName: media-filesystem
  pinning:
    distributed: 1

---
# Service for NFS access (ClusterIP for internal access)
apiVersion: v1
kind: Service
metadata:
  name: rook-nfs-media
  namespace: rook-ceph
  labels:
    app: rook-nfs-media
spec:
  selector:
    app: rook-ceph-nfs
    nfs_cluster: media-nfs
  ports:
  - name: nfs
    port: 2049
    protocol: TCP
    targetPort: 2049
  - name: nfs-udp
    port: 2049
    protocol: UDP
    targetPort: 2049
  - name: rpcbind
    port: 111
    protocol: TCP
    targetPort: 111
  - name: rpcbind-udp
    port: 111
    protocol: UDP
    targetPort: 111
  type: ClusterIP

---
# LoadBalancer Service for external NFS access (optional)
apiVersion: v1
kind: Service
metadata:
  name: rook-nfs-media-external
  namespace: rook-ceph
  labels:
    app: rook-nfs-media-external
spec:
  selector:
    app: rook-ceph-nfs
    nfs_cluster: media-nfs
  ports:
  - name: nfs
    port: 2049
    protocol: TCP
    targetPort: 2049
  - name: nfs-udp
    port: 2049
    protocol: UDP
    targetPort: 2049
  - name: rpcbind
    port: 111
    protocol: TCP
    targetPort: 111
  - name: rpcbind-udp
    port: 111
    protocol: UDP
    targetPort: 111  
    type: LoadBalancer
  # loadBalancerIP: 192.168.55.220  # Disabled to avoid conflict with legacy NFS

---
# Job to initialize media directory structure
apiVersion: batch/v1
kind: Job
metadata:
  name: init-media-directories
  namespace: rook-ceph
spec:
  template:
    spec:
      restartPolicy: OnFailure
      containers:
      - name: init-dirs
        image: busybox:1.36
        command:
        - sh
        - -c
        - |
          set -e
          echo "Creating media directory structure..."
          
          # Create main media directories
          mkdir -p /media/downloads/{complete,incomplete,watch}
          mkdir -p /media/media/{movies,tv,music,books}
          mkdir -p /media/torrents/{movies,tv,music,books}
          
          # Set permissions (assuming UID/GID 2000 for media apps)
          chown -R 2000:2000 /media
          chmod -R 755 /media
          
          echo "Media directory structure created successfully!"
          ls -la /media/        
        volumeMounts:
        - name: media-storage
          mountPath: /media
        securityContext:
          runAsUser: 0  # Run as root to set ownership
          runAsGroup: 0
      securityContext:
        fsGroup: 2000
      volumes:
      - name: media-storage
        persistentVolumeClaim:
          claimName: media-cephfs-pvc

---
# PVC for media storage using CephFS
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: media-cephfs-pvc
  namespace: rook-ceph
spec:
  accessModes:
  - ReadWriteMany
  resources:
    requests:
      storage: 100Gi  # Adjust size as needed
  storageClassName: rook-cephfs
