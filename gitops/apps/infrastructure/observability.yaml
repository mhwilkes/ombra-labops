## Grafana (deploy after datasources backends up)
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: grafana
  namespace: argocd
  annotations:
    argocd.argoproj.io/sync-wave: "2"
spec:
  project: default
  source:
    repoURL: https://grafana.github.io/helm-charts
    chart: grafana
    targetRevision: 10.3.0
    helm:
      values: |
        persistence:
          enabled: true
          size: 5Gi
          storageClassName: ceph-rbd # Adjust if different
        adminUser: admin
        adminPassword: admin123 # TODO: replace via Infisical secret ref / Secret valueFrom
        service:
          type: ClusterIP
        ingress:
          enabled: true
          className: nginx
          annotations:
            cert-manager.io/cluster-issuer: letsencrypt-prod
            nginx.ingress.kubernetes.io/ssl-redirect: "true"
          hosts:
            - grafana.michaelwilkes.dev
          tls:
            - secretName: grafana-tls
              hosts:
                - grafana.michaelwilkes.dev
        datasources:
          datasources.yaml:
            apiVersion: 1
            datasources:
              - name: Mimir
                type: prometheus
                access: proxy
                url: http://mimir-nginx.observability.svc.cluster.local
                isDefault: true
              - name: Loki
                type: loki
                access: proxy
                url: http://loki.observability.svc.cluster.local:3100
              - name: Tempo
                type: tempo
                access: proxy
                url: http://tempo.observability.svc.cluster.local:3100
                jsonData:
                  tracesToLogsV2:
                    datasourceUid: loki
                    filterByTraceID: true
        dashboardProviders:
          dashboardproviders.yaml:
            apiVersion: 1
            providers:
              - name: default
                orgId: 1
                folder: ''
                type: file
                disableDeletion: false
                editable: true
                options:
                  path: /var/lib/grafana/dashboards/default
        dashboards: {}
  destination:
    server: https://kubernetes.default.svc
    namespace: observability
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
    syncOptions:
    - CreateNamespace=true
---
# Mimir (monolithic mode to start)
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: mimir
  namespace: argocd
  annotations:
    argocd.argoproj.io/sync-wave: "0"
spec:
  project: default
  source:
    repoURL: https://grafana.github.io/helm-charts
    chart: mimir-distributed
    targetRevision: 6.0.5 # pin suitable version
    helm:
      values: |
        global:
          extraEnv:
            - name: JAEGER_AGENT_HOST
              value: "tempo.observability.svc.cluster.local"
        runtimeConfig: {}
        mimir:
          structuredConfig:
            common:
              storage:
                backend: filesystem
                filesystem:
                  # Use a sibling directory to the ingester TSDB path (/data/tsdb) to avoid overlap constraint
                  dir: /data/blocks
            # Lean: omit limits (use defaults); add later if needed.
        ingester:
          replicas: 1
          persistentVolume:
            enabled: true
            size: 10Gi
            storageClass: ceph-rbd
          zoneAwareReplication:
            enabled: false
        compactor:
          enabled: false
          replicas: 0
        store_gateway:
          enabled: false
          replicas: 0
        overrides_exporter:
          enabled: false
        alertmanager:
          enabled: false
        ruler:
          enabled: false
        minio:
          enabled: false
        metaMonitoring:
          dashboards:
            enabled: true
            label: grafana_dashboard
            labelValue: "1"
  destination:
    server: https://kubernetes.default.svc
    namespace: observability
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
    syncOptions:
    - CreateNamespace=true
---
# Loki
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: loki
  namespace: argocd
  annotations:
    argocd.argoproj.io/sync-wave: "0"
    argocd.argoproj.io/sync-options: SkipDryRunOnMissingResource=true
spec:
  project: default
  source:
    repoURL: https://grafana.github.io/helm-charts
    chart: loki
    targetRevision: 6.46.0
    helm:
      values: |
        deploymentMode: SingleBinary
        loki:
          commonConfig:
            replication_factor: 1
          storage:
            type: filesystem
          auth_enabled: false
        singleBinary:
          replicas: 1
          persistence:
            enabled: true
            size: 10Gi
            storageClass: ceph-rbd
        gateway:
          enabled: false
        read:
          replicas: 0
        write:
          replicas: 0
        # Disable extras we don't need for lean setup
        lokiCanary:
          enabled: false
        grafanaAgentOperator:
          enabled: false
          crds:
            create: false
  destination:
    server: https://kubernetes.default.svc
    namespace: observability
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
    syncOptions:
    - CreateNamespace=true
---
# Tempo
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: tempo
  namespace: argocd
  annotations:
    argocd.argoproj.io/sync-wave: "0"
spec:
  project: default
  source:
    repoURL: https://grafana.github.io/helm-charts
    chart: tempo
    targetRevision: 1.16.0
    helm:
      values: |
        tempo:
          storage:
            trace:
              backend: local
              local:
                path: /var/tempo/traces
          metricsGenerator:
            enabled: false
          otlp:
            grpc:
              enabled: true
            http:
              enabled: true
        persistence:
          enabled: true
          size: 10Gi
          storageClassName: ceph-rbd
        ingress:
          enabled: false
  destination:
    server: https://kubernetes.default.svc
    namespace: observability
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
    syncOptions:
    - CreateNamespace=true
---
# Promtail
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: promtail
  namespace: argocd
  annotations:
    argocd.argoproj.io/sync-wave: "1"
spec:
  project: default
  source:
    repoURL: https://grafana.github.io/helm-charts
    chart: promtail
    targetRevision: 6.17.1
    helm:
      values: |
        config:
          clients:
            - url: http://loki.observability.svc.cluster.local:3100/loki/api/v1/push
          snippets:
            pipelineStages:
              - cri: {}
        extraScrapeConfigs: []
  destination:
    server: https://kubernetes.default.svc
    namespace: observability
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
    syncOptions:
    - CreateNamespace=true
---
# Grafana Alloy (agent)
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: alloy
  namespace: argocd
  annotations:
    argocd.argoproj.io/sync-wave: "1"
spec:
  project: default
  source:
    repoURL: https://grafana.github.io/helm-charts
    chart: alloy
    targetRevision: 1.5.0
    helm:
      values: |
        alloy:
          extraPorts:
            - name: otlp-grpc
              port: 4317
              targetPort: 4317
              protocol: TCP
            - name: otlp-http
              port: 4318
              targetPort: 4318
              protocol: TCP
          configMap:
            create: true
            content: |
              // Logs collection and forwarding
              loki.source.kubernetes "pods" {
                targets    = discovery.kubernetes.pods.targets
                forward_to = [loki.process.pods.receiver]
              }

              loki.process "pods" {
                forward_to = [loki.write.default.receiver]

                stage.cri {}
                stage.labels {
                  values = {
                    cluster = "home-cluster",
                  }
                }
              }

              loki.write "default" {
                endpoint {
                  url = "http://loki.observability.svc.cluster.local:3100/loki/api/v1/push"
                }
              }

              // Metrics collection and forwarding
              discovery.kubernetes "endpoints" {
                role = "endpoints"
              }

              prometheus.scrape "kube_state_metrics" {
                targets = [
                  {
                    "__address__" = "kube-state-metrics.observability.svc.cluster.local:8080",
                    "job" = "kube-state-metrics",
                  },
                ]
                forward_to     = [prometheus.remote_write.default.receiver]
                scrape_interval = "30s"
              }

              prometheus.scrape "node_exporter" {
                targets = [
                  {
                    "__address__" = "prometheus-node-exporter.observability.svc.cluster.local:9100",
                    "job" = "node-exporter",
                  },
                ]
                forward_to     = [prometheus.remote_write.default.receiver]
                scrape_interval = "30s"
              }

              prometheus.scrape "kubernetes_pods" {
                targets    = discovery.kubernetes.pods.targets
                forward_to = [prometheus.remote_write.default.receiver]
                scrape_interval = "30s"
              }

              prometheus.remote_write "default" {
                endpoint {
                  url = "http://mimir-nginx.observability.svc.cluster.local/api/v1/push"
                }
              }

              // Traces collection and forwarding
              otelcol.receiver.otlp "default" {
                grpc {
                  endpoint = "0.0.0.0:4317"
                }
                http {
                  endpoint = "0.0.0.0:4318"
                }
                output {
                  traces  = [otelcol.exporter.otlp.tempo.input]
                  metrics = [otelcol.exporter.otlp.tempo.input]
                  logs    = [otelcol.exporter.otlp.tempo.input]
                }
              }

              otelcol.exporter.otlp "tempo" {
                client {
                  endpoint = "tempo.observability.svc.cluster.local:4317"
                }
              }

              // Kubernetes discovery
              discovery.kubernetes "pods" {
                role = "pod"
              }
        controller:
          type: daemonset
        rbac:
          create: true
          rules:
            - apiGroups: [""]
              resources: ["nodes", "nodes/proxy", "services", "endpoints", "pods"]
              verbs: ["get", "list", "watch"]
            - apiGroups: ["discovery.k8s.io"]
              resources: ["endpointslices"]
              verbs: ["get", "list", "watch"]
        serviceAccount:
          create: true
  destination:
    server: https://kubernetes.default.svc
    namespace: observability
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
    syncOptions:
    - CreateNamespace=true
---
# kube-state-metrics
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: kube-state-metrics
  namespace: argocd
  annotations:
    argocd.argoproj.io/sync-wave: "1"
spec:
  project: default
  source:
    repoURL: https://prometheus-community.github.io/helm-charts
    chart: kube-state-metrics
    targetRevision: 6.4.2
    helm:
      values: |
        prometheus:
          monitor:
            enabled: false
  destination:
    server: https://kubernetes.default.svc
    namespace: observability
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
    syncOptions:
    - CreateNamespace=true
---
# node-exporter
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: node-exporter
  namespace: argocd
  annotations:
    argocd.argoproj.io/sync-wave: "1"
spec:
  project: default
  source:
    repoURL: https://prometheus-community.github.io/helm-charts
    chart: prometheus-node-exporter
    targetRevision: 4.49.2
    helm:
      values: |
        service:
          port: 9100
        prometheus:
          monitor:
            enabled: false
  destination:
    server: https://kubernetes.default.svc
    namespace: observability
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
    syncOptions:
    - CreateNamespace=true
